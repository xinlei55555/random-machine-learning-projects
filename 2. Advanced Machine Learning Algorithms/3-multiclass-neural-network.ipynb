{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import Sequential\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.layers import Flatten\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "importing the MNIST dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28) [5 0 4 ... 5 6 8]\n"
     ]
    }
   ],
   "source": [
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#make sure that the pixel values are between 0 and 1 \n",
    " #(Since the pixels can be RGB, and RGB goes from 0 to 255)\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "\n",
    "assert x_train.shape == (60000, 28, 28)\n",
    "assert x_test.shape == (10000, 28, 28)\n",
    "assert y_train.shape == (60000,)\n",
    "assert y_test.shape == (10000,)\n",
    "\n",
    "print(x_train.shape, y_train)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## step 1 specify the model.\n",
    "\n",
    "**note the ouput layer has more than 2 units."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential([\n",
    "    #this takes the inputs, which are 28x28 images, and forms 2D single line vectors\n",
    "    Flatten(input_shape=(28,28)),\n",
    "\n",
    "#the number of neurons, just always try 256, 1024 etc etc until 4096\n",
    "    #by James's experience, this is the best\n",
    "    Dense(units=250, activation='relu'),\n",
    "    Dense(units=250, activation='relu'),\n",
    "    # Dense(units=10, activation='softmax')\n",
    "    Dense(units=10, activation='linear')\n",
    "])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# step 2: specify the loss and cost\n",
    "\n",
    "**the cost function is also different, since we are not using binary crossentropy (we don't have only 2 possible outputs, instead we need the piecewise function with the logs for EVERY activation possible)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.losses import SparseCategoricalCrossentropy\n",
    "# model.compile(loss=SparseCategoricalCrossentropy())\n",
    "\n",
    "#this line, from_logits=True, means that the output layer is a linear layer, and the activation function is the softmax function.\n",
    "model.compile(loss=SparseCategoricalCrossentropy(from_logits=True), metrics=[\"accuracy\"])\n",
    "    #the metrics=[\"accuracy\"] is just so that when I train, I can see the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 3.9928e-07 - accuracy: 1.0000 - val_loss: 0.0181 - val_accuracy: 0.9973\n",
      "Epoch 2/10\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 3.7167e-07 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9973\n",
      "Epoch 3/10\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 3.4820e-07 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9973\n",
      "Epoch 4/10\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 3.3045e-07 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 0.9973\n",
      "Epoch 5/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 3.1368e-07 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9973\n",
      "Epoch 6/10\n",
      "1688/1688 [==============================] - 8s 5ms/step - loss: 2.9925e-07 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9973\n",
      "Epoch 7/10\n",
      "1688/1688 [==============================] - 9s 6ms/step - loss: 2.8626e-07 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9973\n",
      "Epoch 8/10\n",
      "1688/1688 [==============================] - 9s 5ms/step - loss: 2.7530e-07 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9973\n",
      "Epoch 9/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 2.6508e-07 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9973\n",
      "Epoch 10/10\n",
      "1688/1688 [==============================] - 10s 6ms/step - loss: 2.5562e-07 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 0.9973\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1589dbad6d0>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(x_train,y_train, epochs=10, validation_split=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 0.1379 - accuracy: 0.9852\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.13794299960136414, 0.9851999878883362]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.evaluate(x_test,y_test)\n",
    "model.summary()\n",
    "    "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RAndom stuff that doesn't relate to the machine learning    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def plot_images(images,labels,img_shape,layout,cmap=\"gray\"):\n",
    "  if len(labels) == 0:\n",
    "    labels = [\"\"] * len(images)\n",
    "  area = layout[0] * layout[1]\n",
    "  rows, cols = layout\n",
    "  fig, ax = plt.subplots(rows,cols,figsize=(rows*2,cols*2))\n",
    "  fig.tight_layout()\n",
    "  # fig.tight_layout(h_pad=0)\n",
    "  for i, subplot in enumerate(ax.flatten()):\n",
    "    if i >= len(images):\n",
    "      break\n",
    "    img = np.reshape(images[i],img_shape)\n",
    "    subplot.imshow(img,cmap=cmap)\n",
    "    subplot.set_title(labels[i])\n",
    "    subplot.set_xticks([])\n",
    "    subplot.set_yticks([])\n",
    "  plt.show()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualizing our models\n",
    "Let's see how well our model performs by drawing digits ourselves! We will use `Gradio`, a library that allows you to make interactive visualizations right inside Colab itself!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7860\n",
      "Running on public URL: https://9f66c396aa83eb0905.gradio.live\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting and GPU upgrades (NEW!), check out Spaces: https://huggingface.co/spaces\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://9f66c396aa83eb0905.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 185ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n"
     ]
    }
   ],
   "source": [
    "#!pip install gradio\n",
    "import gradio as gr\n",
    "def classify(image):\n",
    "  # describe(image,\"image\")\n",
    "  prediction = model.predict(image.reshape(-1,28,28) / 255).tolist()[0]\n",
    "  return {str(i): prediction[i] for i in range(10)}\n",
    "  \n",
    "sketchpad = gr.Image(\n",
    "                  image_mode='L', \n",
    "                  source='canvas', \n",
    "                  shape=(28, 28), \n",
    "                  invert_colors=True, \n",
    "                  tool= 'select')\n",
    "label = gr.Label(num_top_classes=3)\n",
    "interface = gr.Interface(classify, sketchpad, label)\n",
    "interface.launch(share=True,debug=False);"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
